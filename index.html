<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Start SEO -->
  <meta name="description"
    content="Graduate student working on Neuroscience and Deep Learning problems.">
  <meta name="keywords"
    content="Andre Telfer, Telfer, Andre, Ottawa, Canada, Research, Deep Learning, Carleton University, Neuroscience">

  <!-- content schema for crawlers https://schema.org/Person -->
  <!-- Validate here: https://validator.schema.org/ -->
  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "@id": "https://a-telfer.github.io/#person",
      "email": "andretelfer@cmail.carleton.ca",
      "birthPlace" : {
        "@type": "Place",
        "address": {
          "@type": "PostalAddress",
          "addressLocality": "Vancouver",
          "addressRegion": "BC",
          "addressCountry": "Canada"
        }
      },
      "alumniOf": [
        {
          "@type": "CollegeOrUniversity",
          "name": "Carleton University",
          "sameAs": [
            "https://carleton.ca/",
            "https://twitter.com/Carleton_U",
            "https://www.facebook.com/carletonuniversity/",
            "https://ca.linkedin.com/school/carleton-university/"
          ]
        },
        {
          "@type": "CollegeOrUniversity",
          "name": "West Point Grey Academy",
          "sameAs": [
            "https://www.wpga.ca/",
            "https://en.wikipedia.org/wiki/West_Point_Grey_Academy"
          ]
        }
      ],
      "colleague": [
        "http://people.scs.carleton.ca/~olivervankaick/",
        "https://carleton.ca/neuroscience/profile/alfonso-abizaid/",
        "https://science.uottawa.ca/biology/people/lewis-john-e"
      ],
      "knows": [
        {
          "givenName": "Kristen",
          "additionalName": "Yvonne",
          "familyName": "Isherwood",
          "name": "Kristen Yvonne Isherwood",
          "alternateName": "Kris Isherwood",
          "sameAs": "https://www.linkedin.com/in/kristenisherwood/",
          "email": "kyisherwood@gmail.com",
          "gender": "female",
          "address": "Toronto, Ontario, Canada",
          "jobTitle": "Archnemesis",
          "affiliation": {
            "@type": "Organization",
            "name": "Shopify",
            "sameAs": "https://www.shopify.ca"
          }
        }
      ],
      "affiliation": [
        {
          "@type": "Organization",
          "name": "Carleton University",
          "sameAs": [
            "https://carleton.ca/",
            "https://twitter.com/Carleton_U",
            "https://www.facebook.com/carletonuniversity/",
            "https://ca.linkedin.com/school/carleton-university/"
          ]
        },
        {
          "@type": "Organization",
          "name": "SickKids",
          "sameAs": [
            "https://www.sickkids.ca"
          ]
        }
      ],
      "jobTitle": "Machine Learning Specialist", 
      "Description": "AI Research and Deployment",
      "disambiguatingDescription": "Studies animal behaviour using deep learning",
      "image": "https://avatars.githubusercontent.com/u/75952206?v=4",
      "gender": "Male",
      "name": "Andre Telfer",
      "alternateName": "Andre Telfer",
      "givenName": "Andre",
      "familyName": "Telfer",
      "nationality": "Canadian",
      "address": {
        "@type": "PostalAddress",
        "addressLocality": "Ottawa",
        "addressRegion": "Ontario",
        "addressCountry": "Canada"
      },
      "url": "https://a-telfer.github.io/",
      "sameAs": [
        "https://www.linkedin.com/in/andretelfer",
        "https://www.researchgate.net/profile/Andre-Telfer",
        "https://scholar.google.ca/citations?user=Kj37_DMAAAAJ&hl=en",
        "https://github.com/A-Telfer",
        "https://twitter.com/AndreNeuroland",
        "https://devpost.com/andretelfer"
      ]
    }
  </script>

  <!-- Google search console for checking SEO https://search.google.com/search-console/welcome?utm_source=about-page -->
  <meta name="google-site-verification" content="YwxOb5svPyAYpbEgIKCrY4Ey2tz-4ov2WoR7JExH9Ws" />
  <!-- End SEO  -->

  <!-- FavIcons https://favicon.io/favicon-generator/ -->
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">

  <!-- Local CSS -->
  <link rel="stylesheet" href="assets/css/starter.css">
  <title>Andre Telfer</title>
</head>

<body>
  <!-- Navbar -->
  <nav class="navbar navbar-expand-md">
    <div class="container-fluid">
      <a class="navbar-brand" href="/">Andre Telfer </a>
      <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarNav"
        aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="navbar-collapse" id="navbarNav">
        <ul class="navbar-nav">
          <!-- <li class="nav-item">
            <a class="nav-link" href="#profile">{ Research Interest }</a>
          </li> -->
          <li class="nav-item">
            <a class="nav-link" href="#background">{ Background }</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#msc-thesis">{ Thesis }</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#posters">{ Posters }</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" aria-current="page" href="#projects">{ Projects }</a>
          </li>
        </ul>
      </div>
    </div>
  </nav>

  <div class="col-md-8 py-5 px-3 mx-auto">
    <header class="pb-3 mb-5 mt-3" id="header">
      <h1 class="h3 justify-content-center">
        <a href="#header" class="d-flex align-items-center text-decoration-none">
          <!-- <svg width="48" height="48" viewBox="0 0 12.7 12.7" version="1.1" id="svg12390"
            inkscape:version="1.1.2 (0a00cf5339, 2022-02-04)" sodipodi:docname="logo.svg"
            xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
            xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd" xmlns="http://www.w3.org/2000/svg"
            xmlns:svg="http://www.w3.org/2000/svg">
            <g inkscape:label="Layer 1" inkscape:groupmode="layer" id="layer1">
              <g id="g11984" inkscape:transform-center-x="2.5126138" inkscape:transform-center-y="1.7842468"
                transform="matrix(-0.62162725,-0.73245239,-0.73245239,0.62162725,107.93749,22.096524)" />
              <path id="circle5072"
                style="font-variation-settings:normal;vector-effect:none;fill:currentColor;fill-opacity:1;fill-rule:evenodd;stroke-width:0.960684;stroke-linecap:butt;stroke-linejoin:miter;stroke-miterlimit:4;stroke-dasharray:none;stroke-dashoffset:0;stroke-opacity:1;-inkscape-stroke:none;stop-color:#000000"
                d="M 23.707031 1.3476562 A 22.54137 22.54137 0 0 0 1.1660156 23.888672 A 22.54137 22.54137 0 0 0 23.707031 46.431641 A 22.54137 22.54137 0 0 0 46.25 23.888672 A 22.54137 22.54137 0 0 0 23.707031 1.3476562 z M 21.458984 8.4980469 L 25.755859 8.4980469 L 36.431641 36.511719 L 32.492188 36.511719 L 29.939453 29.326172 L 17.3125 29.326172 L 14.759766 36.511719 L 10.763672 36.511719 L 21.458984 8.4980469 z M 23.597656 12.232422 L 18.457031 26.173828 L 28.757812 26.173828 L 23.597656 12.232422 z "
                transform="scale(0.26458333)" />
              <g aria-label="A" id="text10237"
                style="font-size:10.1672px;line-height:1.25;word-spacing:0px;fill:#ffffff;stroke-width:0.254181" />
            </g>
          </svg>&nbsp; -->
          
          <span style="font-size:3rem;">Andre Telfer</span>
        </a>
      </h1>
      <p class="lead">Machine Learning Specialist @ SickKids.</p>
      Contact: <a href="mailto:andretelfer@cmail.carleton.ca?subject=Hi!">andretelfer@cmail.carleton.ca</a>
    </header>

    <h2 id="profile">RESEARCH INTERESTS</h2>
    <div class="row">
      <div class="col-sm-4 mb-4 shadow">
        <div class="card" style="width: 100%;">
          <img class="card-img-top" style="filter: brightness(80%);" src="assets/images/dalle-camera-512.jpg"  alt="Card image cap">
          <h4 class="card-title text-center mt-1">Computer Vision</h4>
        </div>
      </div>

      <div class="col-sm-4 mb-4 shadow">
        <div class="card" style="width: 100%;">
          <img class="card-img-top" src="assets/images/dalle-computer-512.jpg"  alt="Card image cap">
          <h4 class="card-title text-center mt-1">Machine Learning</h4>
        </div>
      </div>

      <div class="col-sm-4 mb-4 shadow">
        <div class="card" style="width: 100%;">
          <img class="card-img-top" src="assets/images/dalle-brain-512.jpg"  alt="Card image cap">
          <h4 class="card-title text-center mt-1">Neuroscience</h4>
        </div>
      </div>
    </div>

    <div class="row mt-3 mb-3">
      <div class="col-lg-4 col-sm-12 mb-3">
        <h3 id="background">BACKGROUND</h3>
        Carleton University, Ottawa, Canada.
        <ul>
          <li>MSc Neuroscience.</li>
          <li>BCs&nbsp;Computer&nbsp;Science, Math&nbsp;Minor.</li>
        </ul>
      </div>

      <div class="col-lg-8 col-sm-12 mb-3">
        <h3 id="msc-thesis">DESCRIPTION</h3>
        <p>
          My research leverages deep learning to solve problems in Health Care and Neuroscience. 
        </p>
      </div>
    </div>

    <div class="row mb-5">
      <h3 id="posters">POSTERS</h3>
      <hr class="my-2" />
      <div class="accordion accordion-flush" id="accordionFlushExample">
        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingOne">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
              data-bs-target="#data-day-2023-poster" aria-expanded="false" aria-controls="data-day-2023-poster">
              DATA DAY 2023
            </button>
          </h2>
          <div id="data-day-2023-poster" class="accordion-collapse collapse" aria-labelledby="flush-headingOne"
            data-bs-parent="#accordionFlushExample">
            <div class="accordion-body">
              <a class="no-highlight" href="assets/images/telfer-dataday9.png" target="blank_">
                <img class="img-fluid" style="filter: brightness(90%);" src="assets/images/telfer-dataday9.jpg" />
              </a>
            </div>
          </div>
        </div>
        

        <div class="accordion-item">
          <h2 class="accordion-header" id="flush-headingOne">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
              data-bs-target="#can-acn-2022-poster" aria-expanded="false" aria-controls="can-acn-2022-poster">
              CAN-ACN 2022
            </button>
          </h2>
          <div id="can-acn-2022-poster" class="accordion-collapse collapse" aria-labelledby="flush-headingOne"
            data-bs-parent="#accordionFlushExample">
            <div class="accordion-body">
              <a class="no-highlight" href="assets/images/telfer-can2022.png" target="blank_">
                <img class="img-fluid" style="filter: brightness(90%);" src="assets/images/telfer-can2022-thumbnail.jpg" />
              </a>
            </div>
          </div>
        </div>

        <div class="accordion-item">
          <h2 class="accordion-header" id="sfn-2021-poster">
            <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
              data-bs-target="#flush-collapseTwo" aria-expanded="false" aria-controls="flush-collapseTwo">
              SFN 2021
            </button>
          </h2>

          <!-- Show accordian by default? -->
          <!-- <div id="flush-collapseTwo" class="accordion-collapse collapse show" aria-labelledby="sfn-2021-poster"
            data-bs-parent="#accordionFlushExample"> -->

            <div id="flush-collapseTwo" class="accordion-collapse collapse" aria-labelledby="sfn-2021-poster"
            data-bs-parent="#accordionFlushExample">
            <div class="accordion-body">
              <div class="accordion-body">
                <a class="no-highlight" href="assets/images/telfer-sfn2021.png" target="blank_">
                  <img class="img-fluid" style="filter: brightness(90%);" src="assets/images/telfer-sfn2021-thumbnail.jpg" />
                </a>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Projects -->
    <h3 id="projects">PROJECTS</h3>

    
    <div id="projects-content" class="row">
      <div class="col-sm-6 col-lg-4 mb-4">
        <a class="no-highlight" href="assets/pdf/EmilyMcColville_IBC_2023.pdf" target="blank_">
          <div class="highlight-on-hover">
            <div class="card">
              <img class="card-img-top" style="height:150px;width:100%;object-fit:cover;object-position:50% 100%;" src="assets/images/cricket-mre.jpg" alt="">
              <div class="card-body">
                <h5 class="card-title">Crickets + AI = Space</h5>
                <p class="card-text">Applied AI to support a study of latent viruses in crickets. Funded by the Canadian Space Agency.
                </p>
              </div>
            </div>
          </div>
        </a>
      </div>

      <div class="col-sm-6 col-lg-4 mb-4">
        <a class="no-highlight" data-bs-toggle="modal" data-bs-target="#digital-emotions-mouse-behavior">
          <div class="highlight-on-hover">

            <div class="card">
              <video class="card-img-top" type="video/webm" autoplay loop>
                <!-- <source src="assets/images/digital-emotions-mouse-behavior.webm" type="video/webm"> -->
                <source src="assets/images/digital-emotions-mouse-behavior-sm.mp4" type="video/mp4">
              </video>
              <div class="card-body">
                <h5 class="card-title">Digital Emotions Artwork</h5>
                <p class="card-text">Emotion is core to the arts, but how do researchers capture it when they want to study it?</p>
              </div>
            </div>
          </div>
        </a>

        <div class="modal fade" id="digital-emotions-mouse-behavior" tabindex="-1" aria-labelledby="digitalEmotionModalLabel" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered modal-lg">
            <div class="modal-content">
              <div class="modal-header">
                <h4 class="modal-title" id="digitalEmotionModalLabel">Digital Emotion</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
              </div>
              <div class="modal-body">

                <h5>Description</h5>
                Recently displayed at Haven's Cafe and the Brain and Mental Health artshow 2023.
              </div>
              <div class="modal-footer">
                <button type="button" class="btn btn-primary" data-bs-dismiss="modal">Close</button>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="col-sm-6 col-lg-4 mb-4">
          <div>
            <div class="card">
              <img class="card-img-top" style="height:150px;object-fit: cover;" src="assets/images/stickers.jpg" alt="">
              <div class="card-body">
                <h5 class="card-title">Happy Mice Stickers</h5>
                <p class="card-text">Grimace scales have been developed to recognize pain facial expressions in many animals including mice.</p>
              </div>
            </div>
          </div>
      </div>

      
      <div class="col-sm-6 col-lg-4 mb-4">
        <div class="card">
          <video class="card-img-top" type="video/webm" autoplay loop>
            <!-- <source src="assets/images/digital-emotions-mouse-behavior.webm" type="video/webm"> -->
            <source src="assets/images/mouse-vision-sm.mp4" type="video/mp4">
          </video>
          <div class="card-body">
            <h5 class="card-title">Visual Features in Social Interaction Tests</h5>
            <p class="card-text">Visual attention is an important way animals keep track of threats and rewards.</p>
          </div>
        </div>
      </div>

      
      <div class="col-sm-6 col-lg-4 mb-4">
        <a class="no-highlight" data-bs-toggle="modal" data-bs-target="#gait-modal">
          <div class="highlight-on-hover">
            <div class="card">
              <img class="card-img-top" style="height:150px;width:100%;object-fit:cover;object-position:50% 100%;" src="assets/images/gait-demo-web.jpg" alt="">
              <div class="card-body">
                <h5 class="card-title">Deep Learning Gait Analysis in the Open Field Test</h5>
                <p class="card-text">Parkinson's model gait analysis in a freely moving environment
                </p>
              </div>
            </div>
          </div>
        </a>

        <div class="modal fade" id="gait-modal" tabindex="-1" aria-labelledby="gaitModalLabel" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered modal-lg">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="gaitModalLabel">Gait Analysis in Mice</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
              </div>
              <div class="modal-body">
                <h6>Description</h6>
                <p>
                  In both humans and mice, gait analysis has been used to observe dysfunction in the brain.
                  Gait analysis has been used to provide a measure of brain dysfunction in Parkinson's Disease,
                  Huntington's Disease, Stroke, and Multiple Sclerosis.
                  Mouse models of these conditions also observe changes gait.

                  Traditional systems for performing gait analysis in mice often require the mice to be placed in a
                  specific environment. For treadmill based systems
                  This project aims at performing automatic gait analysis using deep learning with fewer setup
                  constraints.
                </p>
                <h6>Collaborators</h6>
                <p>Vanessa Wong, Emmerson Borthwick, Dana Wymark, Delenn Hills</p>
                <!-- <h6>Links</h6>
                <p>...</p> -->
              </div>
              <div class="modal-footer">
                <button type="button" class="btn btn-primary" data-bs-dismiss="modal">Close</button>
              </div>
            </div>
          </div>
        </div>
      </div>



      <div class="col-sm-6 col-lg-4 mb-4">
        <a class="no-highlight" data-bs-toggle="modal" data-bs-target="#spiking-neural-docoding-modal">
          <div class="highlight-on-hover">

            <div class="card">
              <video class="card-img-top" type="video/webm" autoplay loop>
                <source src="assets/images/spiking-neuron.webm" type="video/webm">
                <source src="assets/images/spiking-neuron.mp4" type="video/mp4">
              </video>
              <div class="card-body">
                <h5 class="card-title">Decoding neural-behaviour correlates with Spiking Neural Networks</h5>
                <p class="card-text">Predicting movement vectors from neural recordings using Artificial Spiking Neural Network.</p>
              </div>
            </div>
          </div>
        </a>

        <div class="modal fade" id="spiking-neural-docoding-modal" tabindex="-1" aria-labelledby="spikingNeuralDecodingModalLabel" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered modal-lg">
            <div class="modal-content">
              <div class="modal-header">
                <h4 class="modal-title" id="spikingNeuralDecodingModalLabel">Decoding neural-behaviour correlates with Spiking Neural Networks</h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
              </div>
              <div class="modal-body">

                <h5>Description</h5>
                <p>Using an Spiking Neural Network (SNN) with leaky integrate-and-fire dynamics we decode the finger velocity of a primate completing a reaching task from multielectrode array neural recordings.</p>
                <h5>Motivation</h5>
                <p>
                  As the number of companies developing brain-computer interfaces continues to grow (e.g., Neuralink and Synchron), it is critical to investigate low latency and energy-efficient neural decoding methods. We wanted to show that spiking neural networks (SNNs), which can be implemented on neuromorphic chips, can decode such signals and fill this gap.
                </p>
                <p>
                  Additionally, SNNs are closer to the underlying neuron biophysics. Therefore, we hoped that by using realistic neural dynamics, we could learn more useful temporal representations in the data compared to traditional decoding or artificial neural network methods which rely on averaging the number of spikes over time.
                </p>
                
                <h5>Links</h5>
                <ul>
                  <li>
                    <a href="https://devpost.com/software/neural-decoding-with-spiking-neural-networks" target="_blank">DevPost Article </a>
                  </li>
                </ul>
                
                <h5>Collaborators</h5>
                <p>{Michael Stuck, XingYun Wang, Zachary Friedenberger, Emine Topcu-Can} @ University of Ottawa</p>
              </div>
              <div class="modal-footer">
                <button type="button" class="btn btn-primary" data-bs-dismiss="modal">Close</button>
              </div>
            </div>
          </div>
        </div>
      </div>


      <div class="col-sm-6 col-lg-4 mb-4">
        <a class="no-highlight" data-bs-toggle="modal" data-bs-target="#facial-expressions-modal">
          <div class="highlight-on-hover">

            <div class="card">
              <img class="card-img-top" src="assets/images/mouse-facial-expressions-thumbnail.jpg" alt="">
              <div class="card-body">
                <h5 class="card-title">Facial Expressions of Mice</h5>
                <p class="card-text">Recognizing mouse wellness from facial expressions.</p>
              </div>
            </div>
          </div>
        </a>

        <div class="modal fade" id="facial-expressions-modal" tabindex="-1"
          aria-labelledby="facialExpressionsModalLabel" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered modal-lg">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="facialExpressionsModalLabel">Automated Classificatio of Mouse Facial
                  Expressions</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
              </div>
              <div class="modal-body">
                <h6>Description</h6>
                <p>
                  Many species exhibit facial expressions corressponding to different states of emotion and wellbeing.
                  Mice are one such species that exhibit facial expressions and recent work has highlighted the ability
                  for DeepLearning (/AI) to automatically recognize
                  them in free-moving mice<a
                    href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0228059"
                    target="_blank">[1]</a>.
                  Models like these could be widely applied to behavioural research in order to improve animal care.
                  Additionally, it potentially offers a new dimension of emotionality data compared to older methods
                  such as the Open Field Test.
                </p>
                <p>
                  Automatic classification of mouse wellbeing based on facial expressions.
                </p>
                <h6>Collaborators</h6>
                <p>Niek Andresen, Dr. Katharina Hohlbaum, Dr. Alfonso Abizaid</p>
                <h6>Links</h6>
                <a href="https://github.com/A-Telfer/mouse-facial-expressions/tree/main/hohlbaum_black_mouse_dataset"
                  target="_blank">BMv1 Well Being Benchmark</a>
              </div>
              <div class="modal-footer">
                <button type="button" class="btn btn-primary" data-bs-dismiss="modal">Close</button>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="col-sm-6 col-lg-4 mb-4">
          <div class="highlight-on-hover">
            <div class="card">
              <img class="card-img-top" src="assets/images/cvpr-results.jpg" alt="">
              <div class="card-body">
                <h5 class="card-title">Inverse Reinforcement Learning of Mouse Motivation.</h5>
                <p class="card-text">Applying Reinforcement Learning to better understand motivations that drive behavior.</p>
              </div>
            </div>
          </div>
      </div>

      <div class="col-sm-6 col-lg-4 mb-4">
        <a class="no-highlight" target="_blank" href="https://abizaidlabai.uc.r.appspot.com/">
          <div class="highlight-on-hover">
            <div class="card">
              <img class="card-img-top" src="assets/images/mgs-consistency-app-thumbnail.jpg" alt="">
              <div class="card-body">
                <h5 class="card-title">Web application for teaching behavioural scoring.</h5>
                <p class="card-text">Onboard new researchers and improve consistency between labs when scoring the Mouse Grimace Scale.</p>
              </div>
            </div>
          </div>
        </a>
      </div>

      <div class="col-sm-6 col-lg-4 mb-4">
        <a class="no-highlight" data-bs-toggle="modal" data-bs-target="#neuroquery-asd-modal">
          <div class="highlight-on-hover">
            <div class="card">
              <img class="card-img-top" src="./assets/images/brain-atlas.jpg" />
              <div class="card-body">
                <h5 class="card-title">Neuroquery-guided meta-analysis of Autism Spectrum Disorder</h5>
                <p class="card-text">Using predictive models trained on data from thousands of fMRI publications as a tool to guide meta-reviews.</p>
              </div>
            </div>
          </div>
        </a>

        <div class="modal fade" id="neuroquery-asd-modal" tabindex="-1" aria-labelledby="neuroqueryAsdModalLabel" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered modal-lg">
            <div class="modal-content">
              <div class="modal-header">
                <h4 class="modal-title" id="neuroqueryAsdModalLabel"> </h4>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
              </div>
              <div class="modal-body">

                <h5>Description</h5>
                <p>A neuroquery-guided review of Autism Spectrum Disorder. Neuroquery is a query tool which trains a model on thousands of pubmed fMRI papers to identify regions of interest in the brain.</p>
                <p>This work uses Neuroquery to predict regions of interest in the brain for Autism Spectrum Disorder in a variety of contexts. These results are then quantified using brain atlasses and compared in-depth to a small number of relevant papers.</p>
                <h5>Collaborators</h5>
                <p>Yara Mahafza, Mason Irvine, Prof. Argel Aguilar-Valles, Prof. Amedeo D'Angiulli</p>
              </div>
              <div class="modal-footer">
                <button type="button" class="btn btn-primary" data-bs-dismiss="modal">Close</button>
              </div>
            </div>
          </div>
        </div>
      </div>
      
      <div class="col-sm-6 col-lg-4 mb-4">
        <a class="no-highlight" data-bs-toggle="modal" data-bs-target="#oft-modal">
          <div class="highlight-on-hover">

            <div class="card">
              <img class="card-img-top" src="assets/images/oft-dashboard-thumbnail.jpg" alt="">
              <div class="card-body">
                <h5 class="card-title">What comes after DeepLabCut: Open Field Test Analysis</h5>
                <p class="card-text">Simple code for turning DeepLabCut outputs into meaningful results and publishable
                  figures.</p>
              </div>
            </div>
          </div>
        </a>

        <div class="modal fade" id="oft-modal" tabindex="-1" aria-labelledby="oftModalLabel" aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered modal-lg">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="oftModalLabel">Deep-learning scoring of the Open Field Test
                  Expressions</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
              </div>
              <div class="modal-body">
                <h6>Description</h6>
                <p>
                  In Neuroscience, behavioural studies in animals are a key component to understanding how the brain
                  functions to generate complex cognitive, motivational, and emotional processes. Behavioural assays are
                  a cornerstone of this behavioural research, and the Open Field Test (OFT) is a common behavioural
                  assay used to test the anxiolytic effects of drugs, motivation, locomotor activity, fear responses and
                  exploration
                </p>
                <p>
                  Experiments using the OFT generate many hours of video which need to be carefully scored.
                  Manual analysis of these videos is a labour intensive process, in many cases taking weeks, and is
                  prone to reliability/consistency issues.
                  Several commercial automated tools exist for scoring OFT videos. However these tools are very
                  expensive,
                  and may perform worse than more recent Open Source solutions [<a
                    href="https://www.nature.com/articles/s41386-020-0776-y" target="blank_">ref</a>] (for example, they
                  may struggle with wires moving in front of the camera).
                  Furthermore, the Open Field Test is frequently modified and closed-source nature of commercial
                  solution drives many researchers back to manual scoring if their needs are not met in entirety.
                </p>

                <p>
                  This project uses an open source deep learning tool (<a
                    href="https://github.com/DeepLabCut/DeepLabCut">DeepLabCut</a>) in order to automate scoring of OFT
                  videos.
                  We introduce a video calibration step in order to account for differences between videos (current
                  tools favour a fixed setup which many labs using shared experimental spaces do not have) and more
                  accurately translate between pixels and real distance units.
                  We also introduce an API that allows researchers to extract new features from dozens of hours of video
                  in only a few lines of code.
                </p>

                <h6>Collaborators</h6>
                <p>Dr. John Lewis (University of Ottawa), Dr. Oliver van Kaick (Carleton University), Dr. Alfonso
                  Abizaid (Carleton University)</p>

                
                <h6>Links</h6>
                <a
                  href="https://github.com/A-Telfer/bapipe-keypoints/tree/fd24e3c7b16bd9901db95f3bbc46efc6f13268f6">https://github.com/A-Telfer/bapipe-keypoints</a>
              </div>
              <div class="modal-footer">
                <button type="button" class="btn btn-primary" data-bs-dismiss="modal">Close</button>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="col-sm-6 col-lg-4 mb-4">
        <a class="no-highlight" data-bs-toggle="modal" data-bs-target="#microscopy-modal">
          <div class="highlight-on-hover">

            <div class="card">
              <img class="card-img-top"
                src="assets/images/screenshot-napari-zone-thumbnail.jpg" alt="">
              <div class="card-body">
                <h5 class="card-title">Cell Counting using Deep Learning</h5>
                <p class="card-text">Quantifying intense activation of neurons through automatic counting of c-Fos
                  expressing cells.</p>
              </div>
            </div>
          </div>
        </a>
        <div class="modal fade" id="microscopy-modal" tabindex="-1" aria-labelledby="microscopyModalLabel"
          aria-hidden="true">
          <div class="modal-dialog modal-dialog-centered modal-lg">
            <div class="modal-content">
              <div class="modal-header">
                <h5 class="modal-title" id="microscopyModalLabel">Cell Counting using Deep Learning</h5>
                <button type="button" class="btn-close" data-bs-dismiss="modal" aria-label="Close"></button>
              </div>
              <div class="modal-body">
                <h6>Description</h6>
                <p>
                  c-Fos expression can occur in neurons after intense activation, and can provide information on cell
                  growth and plasticity <a href="https://www.frontiersin.org/article/10.3389/fncel.2015.00072"
                    target="_blank">[ref]</a>. Currently, manual cell counting is the gold standard. This is time
                  intensive
                  and often takes many weeks,
                  as experiments can produce hundreds of images, each with hundreds of c-Fos expressing cells.
                </p>
                <p>ImageJ is a tool that has been used to semi-automate cell counting. The downside of this approach is
                  that
                  parameters such as pixel intensity thresholds, cell sizes, and cell shapes may need to be adjusted per
                  image. Additionally, images that contain areas of background noise are difficult to extract accurate
                  cell
                  counts from.</p>
                <p>Here we explore alternative methods for performing cell counting for c-Fos expressing cells.</p>
                <h6>Collaborators</h6>
                <p>Aylar Rahimi, Brenna MacAuley, Andrea Smith, Dr. Alfonso Abizaid</p>
                <h6>Links</h6>
                <a
                  href="https://a-telfer.github.io/cfos-automatic-cell-counting/yolov5/creating-dataset-by-sampling-cell-images.html">
                  YOLO c-Fos Expression Detection
                </a>
              </div>
              <div class="modal-footer">
                <button type="button" class="btn btn-primary" data-bs-dismiss="modal">Close</button>
              </div>
            </div>
          </div>
        </div>
      </div>
      

      <div class="col-sm-6 col-lg-4 mb-4">
        <div class="card">
          <img class="card-img-top" src="assets/images/congenital-blindness-attention.jpg" alt="">
          <div class="card-body">
            <h5 class="card-title">Haptic Attention and Neural Plasticity in Acquired Blindness</h5>
            <p class="card-text">Using deep learning to track focus in an indented image recognition task.</p>
          </div>
        </div>
      </div>

    </div>

    <hr class="my-5">
    
    <div class="row text-muted">
      <div class="col-sm-6 col-xs-12">
        <h5>
          Contact
        </h5>
        <p>Andre Telfer @ <a href="mailto:andretelfer@cmail.carleton.ca">andretelfer@cmail.carleton.ca</a></p>
        <p>Abizaid Lab, Carleton University, Ottawa, Canada</p>
        <!-- <p style="opacity: 0;">Kris Isherwood; Kristen Isherwood, Kris Yvonne</p> -->
      </div>
      <div class="col-sm-6 col-xs-12">
        <h5>
          Labs and collaborations
        </h5>
        <ul>
          <li>Prof. Alfonso Abizaid @&nbsp;<a href="https://abizaidlab.github.io/">Abizaid&nbsp;Lab</a></li>
          <li>Prof. John Lewis @&nbsp;<a href="https://mysite.science.uottawa.ca/jlewis/">Lewis&nbsp;Lab</a></li>
          <li>Prof. Oliver van Kaick @&nbsp;<a href="http://people.scs.carleton.ca/~olivervankaick/">GIGL</a></li>
          <li>Prof. Argel Aguilar-Valles @&nbsp;<a href="https://carleton.ca/neuroscience/profile/argel-aguilar-valles/">AV&nbsp;Lab</a></li>
          <li>Prof. Amedeo D'Angiulli @&nbsp;<a href="https://carleton.ca/neuroscience/profile/amedeo-dangiulli/">NICER</a></li>
          <li>Prof. Shawn Hayley @&nbsp;<a href="https://carleton.ca/neuroscience/profile/shawn-hayley/">Hayley&nbsp;Lab</a></li>
          <li>Emine Topcu-Can @&nbsp;<a href="https://github.com/EmineTopcu">Emine&nbsp;the&nbsp;Student&nbsp;(github)</a></li> 
        </ul>
      </div>
    </div>
  </div>

  <!-- Masonry https://getbootstrap.com/docs/5.0/examples/masonry -->
  <script src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js"
    integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D"
    crossorigin="anonymous"></script>

  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
    crossorigin="anonymous"></script>

  <script src="https://code.jquery.com/jquery-3.6.0.min.js"
    integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script type="module" src="assets/js/starter.js"></script>

  
</body>

</html>